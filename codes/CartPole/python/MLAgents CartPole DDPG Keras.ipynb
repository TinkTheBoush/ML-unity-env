{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:\n",
      "3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 11:27:44) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from keras.layers import Dense, Input, Add, GaussianNoise,Concatenate\n",
    "from keras.optimizers import Adam, SGD, Nadam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "\n",
    "from mlagents.envs import UnityEnvironment\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Python version:\")\n",
    "print(sys.version)\n",
    "\n",
    "# check Python version\n",
    "if (sys.version_info[0] < 3):\n",
    "    raise Exception(\"ERROR: ML-Agents Toolkit (v0.3 onwards) requires Python 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"../env/CartPole\"  # Name of the Unity environment binary to launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:\n",
      "'CartPoleAcamedy' started successfully!\n",
      "Unity Academy name: CartPoleAcamedy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: CartPoleBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space size (per agent): 5\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): [1]\n",
      "        Vector Action descriptions: \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name)\n",
    "\n",
    "# Set the default brain to work with\n",
    "default_brain = env.brain_names[0]\n",
    "brain = env.brains[default_brain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OU_noise():\n",
    "    def __init__(self,action_size,mu=0,theta=0.1,sigma=0.1):\n",
    "        self.action_size = action_size\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_size)*self.mu\n",
    "        #self.shape = np.shape(self.action_size)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_size)*self.mu\n",
    "\n",
    "    def noise(self):\n",
    "        x = self.state\n",
    "        \n",
    "        dx = self.theta * (self.mu - x) + self.sigma * nr.randn(self.action_size[0],self.action_size[1])\n",
    "        self.state = x + dx\n",
    "        #print(self.state)\n",
    "        return self.state\n",
    "\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self, state_size, agent_size,action_size):\n",
    "        self.state_size = state_size\n",
    "        self.agent_size = agent_size\n",
    "        self.action_size = action_size\n",
    "        self.load_model = True\n",
    "        self.Gausian_size = 0.01\n",
    "        self.gard_clip_radious = 100.0\n",
    "\n",
    "        # build networks\n",
    "        self.actor = self.build_actor()\n",
    "        self.actor_target = self.build_actor()\n",
    "        self.critic = self.build_critic()\n",
    "        self.critic_target = self.build_critic()\n",
    "        self.actor_updater = self.actor_optimizer()\n",
    "\n",
    "        self.memory = deque(maxlen=50000)\n",
    "        self.batch_size = 256\n",
    "        self.discount_factor = 0.99\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.999\n",
    "        \n",
    "        self.noiser = OU_noise([agent_size,self.action_size]) #수정한 부분\n",
    "        \n",
    "        if self.load_model:\n",
    "            self.actor.load_weights(\"3DBall_actor.h5\")\n",
    "            self.actor_target.load_weights(\"3DBall_actor.h5\")\n",
    "            self.critic.load_weights(\"3DBall_critic.h5\")\n",
    "            self.critic_target.load_weights(\"3DBall_critic.h5\")\n",
    "\n",
    "    def build_actor(self):\n",
    "        print(\"building actor network\")\n",
    "        input = Input(shape=[self.state_size])\n",
    "        h1 = Dense(512, activation='elu')(input)\n",
    "        h1 = Dense(512, activation='elu')(h1)\n",
    "        h1 = Dense(512, activation='elu')(h1)\n",
    "        h1 = Dense(512, activation='elu')(h1)\n",
    "        h1 = Dense(512, activation='elu')(h1)\n",
    "        h1 = Dense(512, activation='elu')(h1)\n",
    "        h1 = Dense(512, activation='elu')(h1)\n",
    "        action = Dense(self.action_size, activation='tanh')(h1)\n",
    "        actor = Model(inputs=input, outputs=action)\n",
    "        actor.summary()\n",
    "        return actor\n",
    "\n",
    "    def actor_optimizer(self):\n",
    "        actions = self.actor.output\n",
    "        dqda = tf.gradients(self.critic.output, self.critic.input)\n",
    "        loss = actions * tf.clip_by_value(-dqda[1],-self.gard_clip_radious,self.gard_clip_radious) \n",
    "\n",
    "        optimizer = Adam(lr=0.00001)\n",
    "        updates = optimizer.get_updates(self.actor.trainable_weights, [], loss)\n",
    "        train = K.function([self.actor.input, self.critic.input[0],\n",
    "                            self.critic.input[1]], [], updates=updates)\n",
    "        return train\n",
    "\n",
    "    def build_critic(self):\n",
    "        print(\"building critic network\")\n",
    "        state = Input(shape=[self.state_size], name='state_input')\n",
    "        action = Input(shape=[self.action_size], name='action_input')\n",
    "        w1 = Dense(64, activation='elu')(state)\n",
    "        w1 = Dense(64, activation='elu')(w1)\n",
    "        a1 = Dense(64, activation='elu')(action)\n",
    "        a1 = Dense(64, activation='elu')(a1)\n",
    "        c = Concatenate()([w1,a1])\n",
    "        #c = Add()([w1,a1])\n",
    "        h2 = Dense(512, activation='elu')(c)\n",
    "        h2 = Dense(512, activation='elu')(h2)\n",
    "        h2 = Dense(512, activation='elu')(h2)\n",
    "        h2 = Dense(512, activation='elu')(h2)\n",
    "        h2 = Dense(512, activation='elu')(h2)\n",
    "        h2 = Dense(512, activation='elu')(h2)\n",
    "        h2 = Dense(512, activation='elu')(h2)\n",
    "        Velue = Dense(1, activation='linear',kernel_regularizer=regularizers.l1_l2(0.0001,0.0001))(h2)\n",
    "        critic = Model(inputs=[state, action], outputs=Velue)\n",
    "        critic.compile(loss='mse', optimizer=Adam(lr=0.00001))\n",
    "        critic.summary()\n",
    "        return critic\n",
    "\n",
    "    def get_action(self, state):\n",
    "        self.epsilon = max(self.epsilon*self.epsilon_decay,0.05)\n",
    "        #print(state)\n",
    "        action = self.actor.predict(state)\n",
    "\n",
    "        real = action + self.epsilon*self.noiser.noise()\n",
    "        return np.clip(real,-1.1,1.1)\n",
    "    \n",
    "    def gat_action_nonoise(self,state):\n",
    "        action = self.actor.predict(state)\n",
    "        \n",
    "        real = action + 0.01*self.noiser.noise()\n",
    "        return np.clip(real,-1.1,1.1)\n",
    "\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        for i in range(self.agent_size):\n",
    "            self.memory.append((state[i], action[i], reward[i], next_state[i], done[i]))\n",
    "        #self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train_model(self):\n",
    "        # make mini-batch from replay memory\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        states = np.asarray([e[0] for e in mini_batch])\n",
    "        actions = np.asarray([e[1] for e in mini_batch])\n",
    "        rewards = np.asarray([e[2] for e in mini_batch])\n",
    "        next_states = np.asarray([e[3] for e in mini_batch])\n",
    "        dones = np.asarray([e[4] for e in mini_batch])\n",
    "\n",
    "        # update critic network\n",
    "        critic_action_input = self.actor_target.predict(next_states)\n",
    "        target_q_values = self.critic_target.predict([next_states, critic_action_input])\n",
    "\n",
    "        targets = np.zeros([self.batch_size, 1])\n",
    "        for i in range(self.batch_size):\n",
    "            if dones[i]:\n",
    "                targets[i] = rewards[i]\n",
    "            else:\n",
    "                targets[i] = rewards[i] + self.discount_factor * target_q_values[i]\n",
    "\n",
    "        self.critic.train_on_batch([states, actions], targets)\n",
    "\n",
    "        # update actor network\n",
    "        a_for_grad = self.actor.predict(states)\n",
    "        self.actor_updater([states, states, a_for_grad])\n",
    "        #self.actor_updater([states, states, actions])\n",
    "        \n",
    "    def train_critic(self):\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        states = np.asarray([e[0] for e in mini_batch])\n",
    "        actions = np.asarray([e[1] for e in mini_batch])\n",
    "        rewards = np.asarray([e[2] for e in mini_batch])\n",
    "        next_states = np.asarray([e[3] for e in mini_batch])\n",
    "        dones = np.asarray([e[4] for e in mini_batch])\n",
    "        \n",
    "        self.critic.train_on_batch([states, actions], rewards)\n",
    "\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.actor_target.set_weights(self.actor.get_weights())\n",
    "        self.critic_target.set_weights(self.critic.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent state looks like: \n",
      "[0.         0.58897752 0.         0.         0.        ]\n",
      "Agent shape looks like: \n",
      "(10, 5)\n",
      "Agent shape looks like: \n",
      "(0,)\n",
      "building actor network\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               3072      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,579,521\n",
      "Trainable params: 1,579,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "building actor network\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               3072      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,579,521\n",
      "Trainable params: 1,579,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "building critic network\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "state_input (InputLayer)        (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "action_input (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 64)           384         state_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 64)           128         action_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 64)           4160        dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 64)           4160        dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           dense_18[0][0]                   \n",
      "                                                                 dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 512)          66048       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 512)          262656      dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 512)          262656      dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 512)          262656      dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 512)          262656      dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 512)          262656      dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 512)          262656      dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 1)            513         dense_27[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,651,329\n",
      "Trainable params: 1,651,329\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "building critic network\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "state_input (InputLayer)        (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "action_input (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 64)           384         state_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 64)           128         action_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 64)           4160        dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 64)           4160        dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128)          0           dense_30[0][0]                   \n",
      "                                                                 dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 512)          66048       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 512)          262656      dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 512)          262656      dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 512)          262656      dense_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 512)          262656      dense_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 512)          262656      dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 512)          262656      dense_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 1)            513         dense_39[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,651,329\n",
      "Trainable params: 1,651,329\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=True)[default_brain]\n",
    "\n",
    "# Examine the state space for the default brain\n",
    "print(\"Agent state looks like: \\n{}\".format(env_info.vector_observations[0]))\n",
    "\n",
    "print(\"Agent shape looks like: \\n{}\".format(np.shape(env_info.vector_observations)))\n",
    "\n",
    "for observation in env_info.visual_observations:\n",
    "    print(\"Agent observations look like:\")\n",
    "    if observation.shape[3] == 3:\n",
    "        plt.imshow(observation[0,:,:,:])\n",
    "    else:\n",
    "        plt.imshow(observation[0,:,:,0])\n",
    "print(\"Agent shape looks like: \\n{}\".format(np.shape(env_info.visual_observations)))\n",
    "\n",
    "agent = DDPGAgent(5,10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\ml-agent\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\ml-agent\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_0 reward: nan episilon: 0.12617562129402005\n",
      "episode_1 reward: nan episilon: 0.12617562129402005\n",
      "episode_2 reward: 375.99501249790194 episilon: 0.12617562129402005\n",
      "episode_3 reward: 375.99501249790194 episilon: 0.12617562129402005\n",
      "episode_4 reward: 379.53031074404714 episilon: 0.12617562129402005\n",
      "episode_5 reward: 379.53031074404714 episilon: 0.12617562129402005\n",
      "episode_6 reward: 380.30878142118456 episilon: 0.12617562129402005\n",
      "episode_7 reward: 380.30878142118456 episilon: 0.12617562129402005\n",
      "episode_8 reward: 375.1499094069004 episilon: 0.12617562129402005\n",
      "episode_9 reward: 375.1499094069004 episilon: 0.12617562129402005\n",
      "episode_10 reward: 372.993072450161 episilon: 0.12617562129402005\n",
      "episode_11 reward: 372.993072450161 episilon: 0.12617562129402005\n",
      "episode_12 reward: 374.32981864213946 episilon: 0.12617562129402005\n",
      "episode_13 reward: 374.32981864213946 episilon: 0.12617562129402005\n",
      "episode_14 reward: 370.40313653945924 episilon: 0.12617562129402005\n",
      "episode_15 reward: 370.40313653945924 episilon: 0.12617562129402005\n",
      "episode_16 reward: 370.0981539487839 episilon: 0.12617562129402005\n",
      "episode_17 reward: 370.0981539487839 episilon: 0.12617562129402005\n",
      "episode_18 reward: 381.27329538464545 episilon: 0.12617562129402005\n",
      "episode_19 reward: 381.27329538464545 episilon: 0.12617562129402005\n",
      "episode_20 reward: 381.4118568956852 episilon: 0.12617562129402005\n",
      "episode_21 reward: 381.4118568956852 episilon: 0.12617562129402005\n",
      "episode_22 reward: 375.77629532814024 episilon: 0.12617562129402005\n",
      "episode_23 reward: 375.77629532814024 episilon: 0.12617562129402005\n",
      "episode_24 reward: 379.08546989560125 episilon: 0.12617562129402005\n",
      "episode_25 reward: 379.08546989560125 episilon: 0.12617562129402005\n",
      "episode_26 reward: 377.5071676850319 episilon: 0.12617562129402005\n",
      "episode_27 reward: 377.5071676850319 episilon: 0.12617562129402005\n",
      "episode_28 reward: 370.7997372210026 episilon: 0.12617562129402005\n",
      "episode_29 reward: 370.7997372210026 episilon: 0.12617562129402005\n",
      "episode_30 reward: 372.09452974796295 episilon: 0.12617562129402005\n",
      "episode_31 reward: 372.09452974796295 episilon: 0.12617562129402005\n",
      "episode_32 reward: 377.9306610763073 episilon: 0.12617562129402005\n",
      "episode_33 reward: 377.9306610763073 episilon: 0.12617562129402005\n",
      "episode_34 reward: 378.2598197877407 episilon: 0.12617562129402005\n",
      "episode_35 reward: 378.2598197877407 episilon: 0.12617562129402005\n",
      "episode_36 reward: 378.8457837641239 episilon: 0.12617562129402005\n",
      "episode_37 reward: 378.8457837641239 episilon: 0.12617562129402005\n",
      "episode_38 reward: 383.43296921849253 episilon: 0.12617562129402005\n",
      "episode_39 reward: 383.43296921849253 episilon: 0.12617562129402005\n",
      "episode_40 reward: 380.02654404044154 episilon: 0.12617562129402005\n",
      "episode_41 reward: 380.02654404044154 episilon: 0.12617562129402005\n",
      "episode_42 reward: 376.88873265385627 episilon: 0.12617562129402005\n",
      "episode_43 reward: 376.88873265385627 episilon: 0.12617562129402005\n",
      "episode_44 reward: 376.2716192305088 episilon: 0.12617562129402005\n",
      "episode_45 reward: 376.2716192305088 episilon: 0.12617562129402005\n",
      "episode_46 reward: 373.72329780459404 episilon: 0.12617562129402005\n",
      "episode_47 reward: 373.72329780459404 episilon: 0.12617562129402005\n",
      "episode_48 reward: 375.4120356142521 episilon: 0.12617562129402005\n",
      "episode_49 reward: 375.4120356142521 episilon: 0.12617562129402005\n",
      "episode_50 reward: 376.19337823987007 episilon: 0.12617562129402005\n",
      "episode_51 reward: 376.19337823987007 episilon: 0.12617562129402005\n",
      "episode_52 reward: 378.94771470427514 episilon: 0.12617562129402005\n",
      "episode_53 reward: 378.94771470427514 episilon: 0.12617562129402005\n",
      "episode_54 reward: 378.5472908258438 episilon: 0.12617562129402005\n",
      "episode_55 reward: 378.5472908258438 episilon: 0.12617562129402005\n",
      "episode_56 reward: 378.6219977080822 episilon: 0.12617562129402005\n",
      "episode_57 reward: 378.6219977080822 episilon: 0.12617562129402005\n",
      "episode_58 reward: 382.8021207511425 episilon: 0.12617562129402005\n",
      "episode_59 reward: 382.8021207511425 episilon: 0.12617562129402005\n",
      "episode_60 reward: 380.00248871445655 episilon: 0.12617562129402005\n",
      "episode_61 reward: 380.00248871445655 episilon: 0.12617562129402005\n",
      "episode_62 reward: 380.21664360761645 episilon: 0.12617562129402005\n",
      "episode_63 reward: 380.21664360761645 episilon: 0.12617562129402005\n",
      "episode_64 reward: 382.35086835622786 episilon: 0.12617562129402005\n",
      "episode_65 reward: 382.35086835622786 episilon: 0.12617562129402005\n",
      "episode_66 reward: 380.1682233273983 episilon: 0.12617562129402005\n",
      "episode_67 reward: 380.1682233273983 episilon: 0.12617562129402005\n",
      "episode_68 reward: 376.9859426856041 episilon: 0.12617562129402005\n",
      "episode_69 reward: 376.9859426856041 episilon: 0.12617562129402005\n",
      "episode_70 reward: 373.312894320488 episilon: 0.12617562129402005\n",
      "episode_71 reward: 373.312894320488 episilon: 0.12617562129402005\n",
      "episode_72 reward: 377.7577258169651 episilon: 0.12617562129402005\n",
      "episode_73 reward: 377.7577258169651 episilon: 0.12617562129402005\n",
      "episode_74 reward: 380.6912833452225 episilon: 0.12617562129402005\n",
      "episode_75 reward: 380.6912833452225 episilon: 0.12617562129402005\n",
      "episode_76 reward: 380.75689281225203 episilon: 0.12617562129402005\n",
      "episode_77 reward: 380.75689281225203 episilon: 0.12617562129402005\n",
      "episode_78 reward: 381.98654343485833 episilon: 0.12617562129402005\n",
      "episode_79 reward: 381.98654343485833 episilon: 0.12617562129402005\n",
      "episode_80 reward: 380.8285763859749 episilon: 0.12617562129402005\n",
      "episode_81 reward: 380.8285763859749 episilon: 0.12617562129402005\n",
      "episode_82 reward: 381.7485870718956 episilon: 0.12617562129402005\n",
      "episode_83 reward: 381.7485870718956 episilon: 0.12617562129402005\n",
      "episode_84 reward: 381.4145897090435 episilon: 0.12617562129402005\n",
      "episode_85 reward: 381.4145897090435 episilon: 0.12617562129402005\n",
      "episode_86 reward: 382.2233036458492 episilon: 0.12617562129402005\n",
      "episode_87 reward: 382.2233036458492 episilon: 0.12617562129402005\n",
      "episode_88 reward: 382.9257316112518 episilon: 0.12617562129402005\n",
      "episode_89 reward: 382.9257316112518 episilon: 0.12617562129402005\n",
      "episode_90 reward: 380.82855478525164 episilon: 0.12617562129402005\n",
      "episode_91 reward: 380.82855478525164 episilon: 0.12617562129402005\n",
      "episode_92 reward: 374.70406875610354 episilon: 0.12617562129402005\n",
      "episode_93 reward: 374.70406875610354 episilon: 0.12617562129402005\n",
      "episode_94 reward: 369.3799099564552 episilon: 0.12617562129402005\n",
      "episode_95 reward: 369.3799099564552 episilon: 0.12617562129402005\n",
      "episode_96 reward: 373.8640776515007 episilon: 0.12617562129402005\n",
      "episode_97 reward: 373.8640776515007 episilon: 0.12617562129402005\n",
      "episode_98 reward: 373.83242240548134 episilon: 0.12617562129402005\n",
      "episode_99 reward: 373.83242240548134 episilon: 0.12617562129402005\n",
      "episode_100 reward: 370.2078659415245 episilon: 0.12617562129402005\n",
      "episode_101 reward: 370.2078659415245 episilon: 0.12617562129402005\n",
      "episode_102 reward: 376.0044901013374 episilon: 0.12617562129402005\n",
      "episode_103 reward: 376.0044901013374 episilon: 0.12617562129402005\n",
      "episode_104 reward: 374.6960804760456 episilon: 0.12617562129402005\n",
      "episode_105 reward: 374.6960804760456 episilon: 0.12617562129402005\n",
      "episode_106 reward: 376.08655321598053 episilon: 0.12617562129402005\n",
      "episode_107 reward: 376.08655321598053 episilon: 0.12617562129402005\n",
      "episode_108 reward: 380.51710614562035 episilon: 0.12617562129402005\n",
      "episode_109 reward: 380.51710614562035 episilon: 0.12617562129402005\n",
      "episode_110 reward: 377.4127763271332 episilon: 0.12617562129402005\n",
      "episode_111 reward: 377.4127763271332 episilon: 0.12617562129402005\n",
      "episode_112 reward: 378.90938236713407 episilon: 0.12617562129402005\n",
      "episode_113 reward: 378.90938236713407 episilon: 0.12617562129402005\n",
      "episode_114 reward: 376.68467739224434 episilon: 0.12617562129402005\n",
      "episode_115 reward: 376.68467739224434 episilon: 0.12617562129402005\n",
      "episode_116 reward: 374.2993941426277 episilon: 0.12617562129402005\n",
      "episode_117 reward: 374.2993941426277 episilon: 0.12617562129402005\n",
      "episode_118 reward: 378.60132945775985 episilon: 0.12617562129402005\n",
      "episode_119 reward: 378.60132945775985 episilon: 0.12617562129402005\n",
      "episode_120 reward: 377.9886271893978 episilon: 0.12617562129402005\n",
      "episode_121 reward: 377.9886271893978 episilon: 0.12617562129402005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_122 reward: 374.0882230460644 episilon: 0.12617562129402005\n",
      "episode_123 reward: 374.0882230460644 episilon: 0.12617562129402005\n",
      "episode_124 reward: 379.310762155056 episilon: 0.12617562129402005\n",
      "episode_125 reward: 379.310762155056 episilon: 0.12617562129402005\n",
      "episode_126 reward: 376.8824450492859 episilon: 0.12617562129402005\n",
      "episode_127 reward: 376.8824450492859 episilon: 0.12617562129402005\n",
      "episode_128 reward: 374.50164291262627 episilon: 0.12617562129402005\n",
      "episode_129 reward: 374.50164291262627 episilon: 0.12617562129402005\n",
      "episode_130 reward: 378.08746535778045 episilon: 0.12617562129402005\n",
      "episode_131 reward: 378.08746535778045 episilon: 0.12617562129402005\n",
      "episode_132 reward: 375.4950793325901 episilon: 0.12617562129402005\n",
      "episode_133 reward: 375.4950793325901 episilon: 0.12617562129402005\n",
      "episode_134 reward: 381.7012608349323 episilon: 0.12617562129402005\n",
      "episode_135 reward: 381.7012608349323 episilon: 0.12617562129402005\n",
      "episode_136 reward: 381.85872579813 episilon: 0.12617562129402005\n",
      "episode_137 reward: 381.85872579813 episilon: 0.12617562129402005\n",
      "episode_138 reward: 374.29325299859045 episilon: 0.12617562129402005\n",
      "episode_139 reward: 374.29325299859045 episilon: 0.12617562129402005\n",
      "episode_140 reward: 369.8571689367294 episilon: 0.12617562129402005\n",
      "episode_141 reward: 369.8571689367294 episilon: 0.12617562129402005\n",
      "episode_142 reward: 374.88853961825373 episilon: 0.12617562129402005\n",
      "episode_143 reward: 374.88853961825373 episilon: 0.12617562129402005\n",
      "episode_144 reward: 381.45652343034743 episilon: 0.12617562129402005\n",
      "episode_145 reward: 381.45652343034743 episilon: 0.12617562129402005\n",
      "episode_146 reward: 375.03621765375135 episilon: 0.12617562129402005\n",
      "episode_147 reward: 375.03621765375135 episilon: 0.12617562129402005\n",
      "episode_148 reward: 370.5584051668644 episilon: 0.12617562129402005\n",
      "episode_149 reward: 370.5584051668644 episilon: 0.12617562129402005\n",
      "episode_150 reward: 376.8260979890823 episilon: 0.12617562129402005\n",
      "episode_151 reward: 376.8260979890823 episilon: 0.12617562129402005\n",
      "episode_152 reward: 379.33703154921534 episilon: 0.12617562129402005\n",
      "episode_153 reward: 379.33703154921534 episilon: 0.12617562129402005\n",
      "episode_154 reward: 379.16561071276664 episilon: 0.12617562129402005\n",
      "episode_155 reward: 379.16561071276664 episilon: 0.12617562129402005\n",
      "episode_156 reward: 380.94586275219916 episilon: 0.12617562129402005\n",
      "episode_157 reward: 380.94586275219916 episilon: 0.12617562129402005\n",
      "episode_158 reward: 376.5568701386452 episilon: 0.12617562129402005\n",
      "episode_159 reward: 376.5568701386452 episilon: 0.12617562129402005\n",
      "episode_160 reward: 378.0207171738148 episilon: 0.12617562129402005\n",
      "episode_161 reward: 378.0207171738148 episilon: 0.12617562129402005\n",
      "episode_162 reward: 378.8097830295563 episilon: 0.12617562129402005\n",
      "episode_163 reward: 378.8097830295563 episilon: 0.12617562129402005\n",
      "episode_164 reward: 378.7797490894794 episilon: 0.12617562129402005\n",
      "episode_165 reward: 378.7797490894794 episilon: 0.12617562129402005\n",
      "episode_166 reward: 380.54495853185654 episilon: 0.12617562129402005\n",
      "episode_167 reward: 380.54495853185654 episilon: 0.12617562129402005\n",
      "episode_168 reward: 379.6926738023758 episilon: 0.12617562129402005\n",
      "episode_169 reward: 379.6926738023758 episilon: 0.12617562129402005\n",
      "episode_170 reward: 379.16885617375374 episilon: 0.12617562129402005\n",
      "episode_171 reward: 379.16885617375374 episilon: 0.12617562129402005\n",
      "episode_172 reward: 377.87854385375977 episilon: 0.12617562129402005\n",
      "episode_173 reward: 377.87854385375977 episilon: 0.12617562129402005\n",
      "episode_174 reward: 379.7496420502663 episilon: 0.12617562129402005\n",
      "episode_175 reward: 379.7496420502663 episilon: 0.12617562129402005\n",
      "episode_176 reward: 374.05845403075216 episilon: 0.12617562129402005\n",
      "episode_177 reward: 374.05845403075216 episilon: 0.12617562129402005\n",
      "episode_178 reward: 376.945813035965 episilon: 0.12617562129402005\n",
      "episode_179 reward: 376.945813035965 episilon: 0.12617562129402005\n",
      "episode_180 reward: 381.7459100186825 episilon: 0.12617562129402005\n",
      "episode_181 reward: 381.7459100186825 episilon: 0.12617562129402005\n",
      "episode_182 reward: 381.1632612168789 episilon: 0.12617562129402005\n",
      "episode_183 reward: 381.1632612168789 episilon: 0.12617562129402005\n",
      "episode_184 reward: 382.5551972389221 episilon: 0.12617562129402005\n",
      "episode_185 reward: 382.5551972389221 episilon: 0.12617562129402005\n",
      "episode_186 reward: 378.298653459549 episilon: 0.12617562129402005\n",
      "episode_187 reward: 378.298653459549 episilon: 0.12617562129402005\n",
      "episode_188 reward: 373.68610324859617 episilon: 0.12617562129402005\n",
      "episode_189 reward: 373.68610324859617 episilon: 0.12617562129402005\n",
      "episode_190 reward: 373.9626467168331 episilon: 0.12617562129402005\n",
      "episode_191 reward: 373.9626467168331 episilon: 0.12617562129402005\n",
      "episode_192 reward: 377.4786815226078 episilon: 0.12617562129402005\n",
      "episode_193 reward: 377.4786815226078 episilon: 0.12617562129402005\n",
      "episode_194 reward: 380.0851844191551 episilon: 0.12617562129402005\n",
      "episode_195 reward: 380.0851844191551 episilon: 0.12617562129402005\n",
      "episode_196 reward: 382.52827059626577 episilon: 0.12617562129402005\n",
      "episode_197 reward: 382.52827059626577 episilon: 0.12617562129402005\n",
      "episode_198 reward: 379.76058449745176 episilon: 0.12617562129402005\n",
      "episode_199 reward: 379.76058449745176 episilon: 0.12617562129402005\n",
      "episode_200 reward: 377.65951134562494 episilon: 0.12617562129402005\n",
      "episode_201 reward: 377.65951134562494 episilon: 0.12617562129402005\n",
      "episode_202 reward: 377.65951134562494 episilon: 0.12617562129402005\n",
      "episode_203 reward: 378.8558552026749 episilon: 0.12617562129402005\n",
      "episode_204 reward: 378.8558552026749 episilon: 0.12617562129402005\n",
      "episode_205 reward: 377.4229693591595 episilon: 0.12617562129402005\n",
      "episode_206 reward: 377.4229693591595 episilon: 0.12617562129402005\n",
      "episode_207 reward: 376.68325915932655 episilon: 0.12617562129402005\n",
      "episode_208 reward: 376.68325915932655 episilon: 0.12617562129402005\n",
      "episode_209 reward: 378.71265051960944 episilon: 0.12617562129402005\n",
      "episode_210 reward: 378.71265051960944 episilon: 0.12617562129402005\n",
      "episode_211 reward: 375.05814307332037 episilon: 0.12617562129402005\n",
      "episode_212 reward: 375.05814307332037 episilon: 0.12617562129402005\n",
      "episode_213 reward: 373.9766703426838 episilon: 0.12617562129402005\n",
      "episode_214 reward: 373.9766703426838 episilon: 0.12617562129402005\n",
      "episode_215 reward: 377.3217714846134 episilon: 0.12617562129402005\n",
      "episode_216 reward: 377.3217714846134 episilon: 0.12617562129402005\n",
      "episode_217 reward: 379.50597537755965 episilon: 0.12617562129402005\n",
      "episode_218 reward: 379.50597537755965 episilon: 0.12617562129402005\n",
      "episode_219 reward: 383.4196373105049 episilon: 0.12617562129402005\n",
      "episode_220 reward: 383.4196373105049 episilon: 0.12617562129402005\n",
      "episode_221 reward: 387.7524601340294 episilon: 0.12617562129402005\n",
      "episode_222 reward: 387.7524601340294 episilon: 0.12617562129402005\n",
      "episode_223 reward: 382.92473033070564 episilon: 0.12617562129402005\n",
      "episode_224 reward: 382.92473033070564 episilon: 0.12617562129402005\n",
      "episode_225 reward: 379.61321051120757 episilon: 0.12617562129402005\n",
      "episode_226 reward: 379.61321051120757 episilon: 0.12617562129402005\n",
      "episode_227 reward: 379.65915326476096 episilon: 0.12617562129402005\n",
      "episode_228 reward: 379.65915326476096 episilon: 0.12617562129402005\n",
      "episode_229 reward: 376.0311905682087 episilon: 0.12617562129402005\n",
      "episode_230 reward: 376.0311905682087 episilon: 0.12617562129402005\n",
      "episode_231 reward: 377.23562986254694 episilon: 0.12617562129402005\n",
      "episode_232 reward: 377.23562986254694 episilon: 0.12617562129402005\n",
      "episode_233 reward: 380.4447118341923 episilon: 0.12617562129402005\n",
      "episode_234 reward: 380.4447118341923 episilon: 0.12617562129402005\n",
      "episode_235 reward: 379.9773984909058 episilon: 0.12617562129402005\n",
      "episode_236 reward: 379.9773984909058 episilon: 0.12617562129402005\n",
      "episode_237 reward: 380.8856304883957 episilon: 0.12617562129402005\n",
      "episode_238 reward: 380.8856304883957 episilon: 0.12617562129402005\n",
      "episode_239 reward: 379.90278487205507 episilon: 0.12617562129402005\n",
      "episode_240 reward: 379.90278487205507 episilon: 0.12617562129402005\n",
      "episode_241 reward: 378.6995527505875 episilon: 0.12617562129402005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_242 reward: 378.6995527505875 episilon: 0.12617562129402005\n",
      "episode_243 reward: 380.91693609952927 episilon: 0.12617562129402005\n",
      "episode_244 reward: 380.91693609952927 episilon: 0.12617562129402005\n",
      "episode_245 reward: 376.3219992876053 episilon: 0.12617562129402005\n",
      "episode_246 reward: 376.3219992876053 episilon: 0.12617562129402005\n",
      "episode_247 reward: 374.14579213261607 episilon: 0.12617562129402005\n",
      "episode_248 reward: 374.14579213261607 episilon: 0.12617562129402005\n",
      "episode_249 reward: 379.37322015166285 episilon: 0.12617562129402005\n",
      "episode_250 reward: 379.37322015166285 episilon: 0.12617562129402005\n",
      "episode_251 reward: 380.4633460044861 episilon: 0.12617562129402005\n",
      "episode_252 reward: 380.4633460044861 episilon: 0.12617562129402005\n",
      "episode_253 reward: 377.8414687871933 episilon: 0.12617562129402005\n",
      "episode_254 reward: 377.8414687871933 episilon: 0.12617562129402005\n",
      "episode_255 reward: 379.3700046777725 episilon: 0.12617562129402005\n",
      "episode_256 reward: 379.3700046777725 episilon: 0.12617562129402005\n",
      "episode_257 reward: 379.40615597963335 episilon: 0.12617562129402005\n",
      "episode_258 reward: 379.40615597963335 episilon: 0.12617562129402005\n",
      "episode_259 reward: 378.49862164258957 episilon: 0.12617562129402005\n",
      "episode_260 reward: 378.49862164258957 episilon: 0.12617562129402005\n",
      "episode_261 reward: 380.8551914870739 episilon: 0.12617562129402005\n",
      "episode_262 reward: 380.8551914870739 episilon: 0.12617562129402005\n",
      "episode_263 reward: 386.826669728756 episilon: 0.12617562129402005\n",
      "episode_264 reward: 386.826669728756 episilon: 0.12617562129402005\n",
      "episode_265 reward: 387.44513884782793 episilon: 0.12617562129402005\n",
      "episode_266 reward: 387.44513884782793 episilon: 0.12617562129402005\n",
      "episode_267 reward: 383.9304284274578 episilon: 0.12617562129402005\n",
      "episode_268 reward: 383.9304284274578 episilon: 0.12617562129402005\n",
      "episode_269 reward: 381.87248143553734 episilon: 0.12617562129402005\n",
      "episode_270 reward: 381.87248143553734 episilon: 0.12617562129402005\n",
      "episode_271 reward: 380.9796749472618 episilon: 0.12617562129402005\n",
      "episode_272 reward: 380.9796749472618 episilon: 0.12617562129402005\n",
      "episode_273 reward: 380.84553914666174 episilon: 0.12617562129402005\n",
      "episode_274 reward: 380.84553914666174 episilon: 0.12617562129402005\n",
      "episode_275 reward: 376.26045950651167 episilon: 0.12617562129402005\n",
      "episode_276 reward: 376.26045950651167 episilon: 0.12617562129402005\n",
      "episode_277 reward: 375.69146245718 episilon: 0.12617562129402005\n",
      "episode_278 reward: 375.69146245718 episilon: 0.12617562129402005\n",
      "episode_279 reward: 377.8507139146328 episilon: 0.12617562129402005\n",
      "episode_280 reward: 377.8507139146328 episilon: 0.12617562129402005\n",
      "episode_281 reward: 372.68855676054955 episilon: 0.12617562129402005\n",
      "episode_282 reward: 372.68855676054955 episilon: 0.12617562129402005\n",
      "episode_283 reward: 372.4181508600712 episilon: 0.12617562129402005\n",
      "episode_284 reward: 372.4181508600712 episilon: 0.12617562129402005\n",
      "episode_285 reward: 376.5784644961357 episilon: 0.12617562129402005\n",
      "episode_286 reward: 376.5784644961357 episilon: 0.12617562129402005\n",
      "episode_287 reward: 377.90964997410777 episilon: 0.12617562129402005\n",
      "episode_288 reward: 377.90964997410777 episilon: 0.12617562129402005\n",
      "episode_289 reward: 378.6449215710163 episilon: 0.12617562129402005\n",
      "episode_290 reward: 378.6449215710163 episilon: 0.12617562129402005\n",
      "episode_291 reward: 378.06968398094176 episilon: 0.12617562129402005\n",
      "episode_292 reward: 378.06968398094176 episilon: 0.12617562129402005\n",
      "episode_293 reward: 380.2028316438198 episilon: 0.12617562129402005\n",
      "episode_294 reward: 380.2028316438198 episilon: 0.12617562129402005\n",
      "episode_295 reward: 381.6479556322098 episilon: 0.12617562129402005\n",
      "episode_296 reward: 381.6479556322098 episilon: 0.12617562129402005\n",
      "episode_297 reward: 376.3270660221577 episilon: 0.12617562129402005\n",
      "episode_298 reward: 376.3270660221577 episilon: 0.12617562129402005\n",
      "episode_299 reward: 372.57409528493883 episilon: 0.12617562129402005\n",
      "episode_300 reward: 372.57409528493883 episilon: 0.12617562129402005\n",
      "episode_301 reward: 376.2893241107464 episilon: 0.12617562129402005\n",
      "episode_302 reward: 376.2893241107464 episilon: 0.12617562129402005\n",
      "episode_303 reward: 375.945351600647 episilon: 0.12617562129402005\n",
      "episode_304 reward: 375.945351600647 episilon: 0.12617562129402005\n",
      "episode_305 reward: 374.484408646822 episilon: 0.12617562129402005\n",
      "episode_306 reward: 374.484408646822 episilon: 0.12617562129402005\n",
      "episode_307 reward: 375.8674973011017 episilon: 0.12617562129402005\n",
      "episode_308 reward: 375.8674973011017 episilon: 0.12617562129402005\n",
      "episode_309 reward: 377.4574211657047 episilon: 0.12617562129402005\n",
      "episode_310 reward: 377.4574211657047 episilon: 0.12617562129402005\n",
      "episode_311 reward: 378.34196560382844 episilon: 0.12617562129402005\n",
      "episode_312 reward: 378.34196560382844 episilon: 0.12617562129402005\n",
      "episode_313 reward: 380.5036942005157 episilon: 0.12617562129402005\n",
      "episode_314 reward: 380.5036942005157 episilon: 0.12617562129402005\n",
      "episode_315 reward: 381.42851747870446 episilon: 0.12617562129402005\n",
      "episode_316 reward: 381.42851747870446 episilon: 0.12617562129402005\n",
      "episode_317 reward: 378.6308332324028 episilon: 0.12617562129402005\n",
      "episode_318 reward: 378.6308332324028 episilon: 0.12617562129402005\n",
      "episode_319 reward: 380.11267748475075 episilon: 0.12617562129402005\n",
      "episode_320 reward: 380.11267748475075 episilon: 0.12617562129402005\n",
      "episode_321 reward: 380.4772903859615 episilon: 0.12617562129402005\n",
      "episode_322 reward: 380.4772903859615 episilon: 0.12617562129402005\n",
      "episode_323 reward: 382.52673161625864 episilon: 0.12617562129402005\n",
      "episode_324 reward: 382.52673161625864 episilon: 0.12617562129402005\n",
      "episode_325 reward: 382.21440041065216 episilon: 0.12617562129402005\n",
      "episode_326 reward: 382.21440041065216 episilon: 0.12617562129402005\n",
      "episode_327 reward: 378.0692670047283 episilon: 0.12617562129402005\n",
      "episode_328 reward: 378.0692670047283 episilon: 0.12617562129402005\n",
      "episode_329 reward: 377.8912109792233 episilon: 0.12617562129402005\n",
      "episode_330 reward: 377.8912109792233 episilon: 0.12617562129402005\n",
      "episode_331 reward: 380.166916435957 episilon: 0.12617562129402005\n",
      "episode_332 reward: 380.166916435957 episilon: 0.12617562129402005\n",
      "episode_333 reward: 382.4735183179379 episilon: 0.12617562129402005\n",
      "episode_334 reward: 382.4735183179379 episilon: 0.12617562129402005\n",
      "episode_335 reward: 379.6426330924034 episilon: 0.12617562129402005\n",
      "episode_336 reward: 379.6426330924034 episilon: 0.12617562129402005\n",
      "episode_337 reward: 376.03650431632997 episilon: 0.12617562129402005\n",
      "episode_338 reward: 376.03650431632997 episilon: 0.12617562129402005\n",
      "episode_339 reward: 374.024020165205 episilon: 0.12617562129402005\n",
      "episode_340 reward: 374.024020165205 episilon: 0.12617562129402005\n",
      "episode_341 reward: 378.4778243005276 episilon: 0.12617562129402005\n",
      "episode_342 reward: 378.4778243005276 episilon: 0.12617562129402005\n",
      "episode_343 reward: 385.4136855065823 episilon: 0.12617562129402005\n",
      "episode_344 reward: 385.4136855065823 episilon: 0.12617562129402005\n",
      "episode_345 reward: 384.94135947227477 episilon: 0.12617562129402005\n",
      "episode_346 reward: 384.94135947227477 episilon: 0.12617562129402005\n",
      "episode_347 reward: 382.273150652647 episilon: 0.12617562129402005\n",
      "episode_348 reward: 382.273150652647 episilon: 0.12617562129402005\n",
      "episode_349 reward: 377.22014690041544 episilon: 0.12617562129402005\n",
      "episode_350 reward: 377.22014690041544 episilon: 0.12617562129402005\n",
      "episode_351 reward: 369.7859546661377 episilon: 0.12617562129402005\n",
      "episode_352 reward: 369.7859546661377 episilon: 0.12617562129402005\n",
      "episode_353 reward: 375.8275268614292 episilon: 0.12617562129402005\n",
      "episode_354 reward: 375.8275268614292 episilon: 0.12617562129402005\n",
      "episode_355 reward: 384.41306510567665 episilon: 0.12617562129402005\n",
      "episode_356 reward: 384.41306510567665 episilon: 0.12617562129402005\n",
      "episode_357 reward: 380.2788510739803 episilon: 0.12617562129402005\n",
      "episode_358 reward: 380.2788510739803 episilon: 0.12617562129402005\n",
      "episode_359 reward: 377.4243890762329 episilon: 0.12617562129402005\n",
      "episode_360 reward: 377.4243890762329 episilon: 0.12617562129402005\n",
      "episode_361 reward: 378.8375299870968 episilon: 0.12617562129402005\n",
      "episode_362 reward: 378.8375299870968 episilon: 0.12617562129402005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_363 reward: 380.8071120917797 episilon: 0.12617562129402005\n",
      "episode_364 reward: 380.8071120917797 episilon: 0.12617562129402005\n",
      "episode_365 reward: 380.3968954324722 episilon: 0.12617562129402005\n",
      "episode_366 reward: 380.3968954324722 episilon: 0.12617562129402005\n",
      "episode_367 reward: 373.1217606782913 episilon: 0.12617562129402005\n",
      "episode_368 reward: 373.1217606782913 episilon: 0.12617562129402005\n",
      "episode_369 reward: 375.4556148707867 episilon: 0.12617562129402005\n",
      "episode_370 reward: 375.4556148707867 episilon: 0.12617562129402005\n",
      "episode_371 reward: 381.3254927456379 episilon: 0.12617562129402005\n",
      "episode_372 reward: 381.3254927456379 episilon: 0.12617562129402005\n",
      "episode_373 reward: 379.29953083992007 episilon: 0.12617562129402005\n",
      "episode_374 reward: 379.29953083992007 episilon: 0.12617562129402005\n",
      "episode_375 reward: 382.5432978451252 episilon: 0.12617562129402005\n",
      "episode_376 reward: 382.5432978451252 episilon: 0.12617562129402005\n",
      "episode_377 reward: 379.04965872764586 episilon: 0.12617562129402005\n",
      "episode_378 reward: 379.04965872764586 episilon: 0.12617562129402005\n",
      "episode_379 reward: 374.82410512566565 episilon: 0.12617562129402005\n",
      "episode_380 reward: 374.82410512566565 episilon: 0.12617562129402005\n",
      "episode_381 reward: 375.7014111816883 episilon: 0.12617562129402005\n",
      "episode_382 reward: 375.7014111816883 episilon: 0.12617562129402005\n",
      "episode_383 reward: 374.13624400496485 episilon: 0.12617562129402005\n",
      "episode_384 reward: 374.13624400496485 episilon: 0.12617562129402005\n",
      "episode_385 reward: 372.850053191185 episilon: 0.12617562129402005\n",
      "episode_386 reward: 372.850053191185 episilon: 0.12617562129402005\n",
      "episode_387 reward: 375.0660578906536 episilon: 0.12617562129402005\n",
      "episode_388 reward: 375.0660578906536 episilon: 0.12617562129402005\n",
      "episode_389 reward: 377.14074858427045 episilon: 0.12617562129402005\n",
      "episode_390 reward: 377.14074858427045 episilon: 0.12617562129402005\n",
      "episode_391 reward: 377.94302659630773 episilon: 0.12617562129402005\n",
      "episode_392 reward: 377.94302659630773 episilon: 0.12617562129402005\n",
      "episode_393 reward: 379.83279155492784 episilon: 0.12617562129402005\n",
      "episode_394 reward: 379.83279155492784 episilon: 0.12617562129402005\n",
      "episode_395 reward: 379.2677995085716 episilon: 0.12617562129402005\n",
      "episode_396 reward: 379.2677995085716 episilon: 0.12617562129402005\n",
      "episode_397 reward: 376.46093823313714 episilon: 0.12617562129402005\n",
      "episode_398 reward: 376.46093823313714 episilon: 0.12617562129402005\n",
      "episode_399 reward: 376.0486349880695 episilon: 0.12617562129402005\n",
      "episode_400 reward: 376.0486349880695 episilon: 0.12617562129402005\n",
      "episode_401 reward: 378.1887485444546 episilon: 0.12617562129402005\n",
      "episode_402 reward: 378.1887485444546 episilon: 0.12617562129402005\n",
      "episode_403 reward: 378.1887485444546 episilon: 0.12617562129402005\n",
      "episode_404 reward: 382.39892354011533 episilon: 0.12617562129402005\n",
      "episode_405 reward: 382.39892354011533 episilon: 0.12617562129402005\n",
      "episode_406 reward: 383.40252445340155 episilon: 0.12617562129402005\n",
      "episode_407 reward: 383.40252445340155 episilon: 0.12617562129402005\n",
      "episode_408 reward: 377.97301893830297 episilon: 0.12617562129402005\n",
      "episode_409 reward: 377.97301893830297 episilon: 0.12617562129402005\n",
      "episode_410 reward: 377.9053489387035 episilon: 0.12617562129402005\n",
      "episode_411 reward: 377.9053489387035 episilon: 0.12617562129402005\n",
      "episode_412 reward: 374.0665659546852 episilon: 0.12617562129402005\n",
      "episode_413 reward: 374.0665659546852 episilon: 0.12617562129402005\n",
      "episode_414 reward: 365.32620840668676 episilon: 0.12617562129402005\n",
      "episode_415 reward: 365.32620840668676 episilon: 0.12617562129402005\n",
      "episode_416 reward: 368.2996200144291 episilon: 0.12617562129402005\n",
      "episode_417 reward: 368.2996200144291 episilon: 0.12617562129402005\n",
      "episode_418 reward: 375.68767898082734 episilon: 0.12617562129402005\n",
      "episode_419 reward: 375.68767898082734 episilon: 0.12617562129402005\n",
      "episode_420 reward: 379.37364848852155 episilon: 0.12617562129402005\n",
      "episode_421 reward: 379.37364848852155 episilon: 0.12617562129402005\n",
      "episode_422 reward: 380.48793972730635 episilon: 0.12617562129402005\n",
      "episode_423 reward: 380.48793972730635 episilon: 0.12617562129402005\n",
      "episode_424 reward: 371.96674938201903 episilon: 0.12617562129402005\n",
      "episode_425 reward: 371.96674938201903 episilon: 0.12617562129402005\n",
      "episode_426 reward: 369.98355677723885 episilon: 0.12617562129402005\n",
      "episode_427 reward: 369.98355677723885 episilon: 0.12617562129402005\n",
      "episode_428 reward: 379.46211420893667 episilon: 0.12617562129402005\n",
      "episode_429 reward: 379.46211420893667 episilon: 0.12617562129402005\n",
      "episode_430 reward: 385.0907735943794 episilon: 0.12617562129402005\n",
      "episode_431 reward: 385.0907735943794 episilon: 0.12617562129402005\n",
      "episode_432 reward: 376.33868737220763 episilon: 0.12617562129402005\n",
      "episode_433 reward: 376.33868737220763 episilon: 0.12617562129402005\n",
      "episode_434 reward: 368.94880174994466 episilon: 0.12617562129402005\n",
      "episode_435 reward: 368.94880174994466 episilon: 0.12617562129402005\n",
      "episode_436 reward: 374.9278977036476 episilon: 0.12617562129402005\n",
      "episode_437 reward: 374.9278977036476 episilon: 0.12617562129402005\n",
      "episode_438 reward: 380.44405311346054 episilon: 0.12617562129402005\n",
      "episode_439 reward: 380.44405311346054 episilon: 0.12617562129402005\n",
      "episode_440 reward: 377.1395482301712 episilon: 0.12617562129402005\n",
      "episode_441 reward: 377.1395482301712 episilon: 0.12617562129402005\n",
      "episode_442 reward: 372.4301676630974 episilon: 0.12617562129402005\n",
      "episode_443 reward: 372.4301676630974 episilon: 0.12617562129402005\n",
      "episode_444 reward: 372.4130911290646 episilon: 0.12617562129402005\n",
      "episode_445 reward: 372.4130911290646 episilon: 0.12617562129402005\n",
      "episode_446 reward: 374.1396640300751 episilon: 0.12617562129402005\n",
      "episode_447 reward: 374.1396640300751 episilon: 0.12617562129402005\n",
      "episode_448 reward: 377.6554045379162 episilon: 0.12617562129402005\n",
      "episode_449 reward: 377.6554045379162 episilon: 0.12617562129402005\n",
      "episode_450 reward: 381.53604965806005 episilon: 0.12617562129402005\n",
      "episode_451 reward: 381.53604965806005 episilon: 0.12617562129402005\n",
      "episode_452 reward: 382.5298587977886 episilon: 0.12617562129402005\n",
      "episode_453 reward: 382.5298587977886 episilon: 0.12617562129402005\n",
      "episode_454 reward: 381.65689380168914 episilon: 0.12617562129402005\n",
      "episode_455 reward: 381.65689380168914 episilon: 0.12617562129402005\n",
      "episode_456 reward: 381.927639490366 episilon: 0.12617562129402005\n",
      "episode_457 reward: 381.927639490366 episilon: 0.12617562129402005\n",
      "episode_458 reward: 382.0685579836369 episilon: 0.12617562129402005\n",
      "episode_459 reward: 382.0685579836369 episilon: 0.12617562129402005\n",
      "episode_460 reward: 383.95260339975357 episilon: 0.12617562129402005\n",
      "episode_461 reward: 383.95260339975357 episilon: 0.12617562129402005\n",
      "episode_462 reward: 382.2747243821621 episilon: 0.12617562129402005\n",
      "episode_463 reward: 382.2747243821621 episilon: 0.12617562129402005\n",
      "episode_464 reward: 382.24831091165544 episilon: 0.12617562129402005\n",
      "episode_465 reward: 382.24831091165544 episilon: 0.12617562129402005\n",
      "episode_466 reward: 380.94946586489675 episilon: 0.12617562129402005\n",
      "episode_467 reward: 380.94946586489675 episilon: 0.12617562129402005\n",
      "episode_468 reward: 378.93123081326485 episilon: 0.12617562129402005\n",
      "episode_469 reward: 378.93123081326485 episilon: 0.12617562129402005\n",
      "episode_470 reward: 378.9569214224815 episilon: 0.12617562129402005\n",
      "episode_471 reward: 378.9569214224815 episilon: 0.12617562129402005\n",
      "episode_472 reward: 375.1787926852703 episilon: 0.12617562129402005\n",
      "episode_473 reward: 375.1787926852703 episilon: 0.12617562129402005\n",
      "episode_474 reward: 379.59069989323615 episilon: 0.12617562129402005\n",
      "episode_475 reward: 379.59069989323615 episilon: 0.12617562129402005\n",
      "episode_476 reward: 380.7243674874306 episilon: 0.12617562129402005\n",
      "episode_477 reward: 380.7243674874306 episilon: 0.12617562129402005\n",
      "episode_478 reward: 376.89193454384804 episilon: 0.12617562129402005\n",
      "episode_479 reward: 376.89193454384804 episilon: 0.12617562129402005\n",
      "episode_480 reward: 379.79165012836455 episilon: 0.12617562129402005\n",
      "episode_481 reward: 379.79165012836455 episilon: 0.12617562129402005\n",
      "episode_482 reward: 381.1709763407707 episilon: 0.12617562129402005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_483 reward: 381.1709763407707 episilon: 0.12617562129402005\n",
      "episode_484 reward: 373.3446085035801 episilon: 0.12617562129402005\n",
      "episode_485 reward: 373.3446085035801 episilon: 0.12617562129402005\n",
      "episode_486 reward: 371.52071564793584 episilon: 0.12617562129402005\n",
      "episode_487 reward: 371.52071564793584 episilon: 0.12617562129402005\n",
      "episode_488 reward: 377.12317610383036 episilon: 0.12617562129402005\n",
      "episode_489 reward: 377.12317610383036 episilon: 0.12617562129402005\n",
      "episode_490 reward: 374.114218711853 episilon: 0.12617562129402005\n",
      "episode_491 reward: 374.114218711853 episilon: 0.12617562129402005\n",
      "episode_492 reward: 378.73018830418584 episilon: 0.12617562129402005\n",
      "episode_493 reward: 378.73018830418584 episilon: 0.12617562129402005\n",
      "episode_494 reward: 384.3487518787384 episilon: 0.12617562129402005\n",
      "episode_495 reward: 384.3487518787384 episilon: 0.12617562129402005\n",
      "episode_496 reward: 379.797479981184 episilon: 0.12617562129402005\n"
     ]
    }
   ],
   "source": [
    "train_mode = False  # Whether to run the environment in training or inference mode\n",
    "\n",
    "reward_memory = deque(maxlen=20)\n",
    "agents_reward = np.zeros(agent.agent_size)\n",
    "env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "for episode in range(10000000):\n",
    "    #env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "    state = env_info.vector_observations\n",
    "    done = False\n",
    "    episode_rewards = 0\n",
    "    for i in range(100):\n",
    "        \n",
    "        if not train_mode:\n",
    "            action = agent.gat_action_nonoise(state)\n",
    "        else:\n",
    "            action = agent.get_action(state)\n",
    "        #action = np.column_stack([np.random.randint(0, action_size[i], size=(len(env_info.agents))) for i in range(len(action_size))])\n",
    "        env_info = env.step(action)[default_brain]\n",
    "        next_state = env_info.vector_observations\n",
    "        reward = env_info.rewards\n",
    "        agents_reward += reward\n",
    "        #episode_rewards += reward#env_info.rewards[0]\n",
    "        done = env_info.local_done\n",
    "        for idx,don in enumerate(done):\n",
    "            if don:\n",
    "                reward_memory.append(agents_reward[idx])\n",
    "                agents_reward[idx] = 0\n",
    "        if train_mode:\n",
    "            agent.append_sample(state,action,reward,next_state,done)\n",
    "        state = next_state\n",
    "        \n",
    "        if len(agent.memory) > agent.batch_size*10 and train_mode:\n",
    "            agent.train_model()\n",
    "        \n",
    "    agent.noiser.reset()\n",
    "    agent.update_target_model()\n",
    "    if agent.epsilon <= 0.05:\n",
    "        agent.epsilon = 0.99\n",
    "\n",
    "    if episode%50 == 0 and not episode == 0 and train_mode:\n",
    "        agent.actor.save(\"3DBall_actor.h5\")\n",
    "        agent.critic.save(\"3DBall_critic.h5\")\n",
    "        print(\"model saved\")\n",
    "    print(\"episode_{} reward: {} episilon: {}\".format(episode,np.mean(reward_memory),agent.epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.epsilon_decay =0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
