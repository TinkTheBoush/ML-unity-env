{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:\n",
      "3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 11:27:44) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from keras.layers import Dense, Input, Add, GaussianNoise,Concatenate\n",
    "from keras.optimizers import Adam, SGD, Nadam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "\n",
    "from mlagents.envs import UnityEnvironment\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Python version:\")\n",
    "print(sys.version)\n",
    "\n",
    "# check Python version\n",
    "if (sys.version_info[0] < 3):\n",
    "    raise Exception(\"ERROR: ML-Agents Toolkit (v0.3 onwards) requires Python 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"../env/CartPole\"  # Name of the Unity environment binary to launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:\n",
      "'CartPoleAcamedy' started successfully!\n",
      "Unity Academy name: CartPoleAcamedy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: CartPoleBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space size (per agent): 5\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): [1]\n",
      "        Vector Action descriptions: \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name)\n",
    "\n",
    "# Set the default brain to work with\n",
    "default_brain = env.brain_names[0]\n",
    "brain = env.brains[default_brain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OU_noise():\n",
    "    def __init__(self,action_size,mu=0,theta=0.03,sigma=0.1):\n",
    "        self.action_size = action_size\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_size)*self.mu\n",
    "        #self.shape = np.shape(self.action_size)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_size)*self.mu\n",
    "\n",
    "    def noise(self):\n",
    "        x = self.state\n",
    "        \n",
    "        dx = self.theta * (self.mu - x) + self.sigma * nr.randn(self.action_size[0],self.action_size[1])\n",
    "        self.state = x + dx\n",
    "        #print(self.state)\n",
    "        return self.state\n",
    "\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self, state_size, agent_size,action_size):\n",
    "        self.state_size = state_size\n",
    "        self.agent_size = agent_size\n",
    "        self.action_size = action_size\n",
    "        self.load_model = False\n",
    "        self.Gausian_size = 0.01\n",
    "        self.gard_clip_radious = 100.0\n",
    "\n",
    "        # build networks\n",
    "        self.actor = self.build_actor()\n",
    "        self.actor_target = self.build_actor()\n",
    "        self.critic = self.build_critic()\n",
    "        self.critic_target = self.build_critic()\n",
    "        self.actor_updater = self.actor_optimizer()\n",
    "\n",
    "        self.memory = deque(maxlen=50000)\n",
    "        self.batch_size = 64\n",
    "        self.discount_factor = 0.99\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.99999\n",
    "        \n",
    "        self.noiser = OU_noise([agent_size,self.action_size]) #수정한 부분\n",
    "        \n",
    "        if self.load_model:\n",
    "            self.actor.load_weights(\"3DBall_actor.h5\")\n",
    "            self.actor_target.load_weights(\"3DBall_actor.h5\")\n",
    "            self.critic.load_weights(\"3DBall_critic.h5\")\n",
    "            self.critic_target.load_weights(\"3DBall_critic.h5\")\n",
    "\n",
    "    def build_actor(self):\n",
    "        print(\"building actor network\")\n",
    "        input = Input(shape=[self.state_size])\n",
    "        h1 = Dense(64, activation='elu')(input)\n",
    "        h1 = Dense(64, activation='elu')(h1)\n",
    "        h1 = Dense(64, activation='elu')(h1)\n",
    "        h1 = Dense(64, activation='elu')(h1)\n",
    "        h1 = Dense(64, activation='elu')(h1)\n",
    "        h1 = Dense(64, activation='elu')(h1)\n",
    "        h1 = Dense(64, activation='elu')(h1)\n",
    "        action = Dense(self.action_size, activation='tanh')(h1)\n",
    "        actor = Model(inputs=input, outputs=action)\n",
    "        actor.summary()\n",
    "        return actor\n",
    "\n",
    "    def actor_optimizer(self):\n",
    "        actions = self.actor.output\n",
    "        dqda = tf.gradients(self.critic.output, self.critic.input)\n",
    "        loss = actions * tf.clip_by_value(-dqda[1],-self.gard_clip_radious,self.gard_clip_radious) #fit in 50\n",
    "\n",
    "        optimizer = Adam(lr=0.00001)\n",
    "        updates = optimizer.get_updates(self.actor.trainable_weights, [], loss)\n",
    "        train = K.function([self.actor.input, self.critic.input[0],\n",
    "                            self.critic.input[1]], [], updates=updates)\n",
    "        return train\n",
    "\n",
    "    def build_critic(self):\n",
    "        print(\"building critic network\")\n",
    "        state = Input(shape=[self.state_size], name='state_input')\n",
    "        action = Input(shape=[self.action_size], name='action_input')\n",
    "        w1 = Dense(64, activation='elu')(state)\n",
    "        w1 = Dense(64, activation='elu')(w1)\n",
    "        a1 = Dense(64, activation='elu')(action)\n",
    "        a1 = Dense(64, activation='elu')(a1)\n",
    "        c = Concatenate()([w1,a1])\n",
    "        #c = Add()([w1,a1])\n",
    "        h2 = Dense(64, activation='elu')(c)\n",
    "        h2 = Dense(64, activation='elu')(h2)\n",
    "        h2 = Dense(64, activation='elu')(h2)\n",
    "        h2 = Dense(64, activation='elu')(h2)\n",
    "        h2 = Dense(64, activation='elu')(h2)\n",
    "        h2 = Dense(64, activation='elu')(h2)\n",
    "        h2 = Dense(64, activation='elu')(h2)\n",
    "        Velue = Dense(1, activation='linear')(h2)\n",
    "        critic = Model(inputs=[state, action], outputs=Velue)\n",
    "        critic.compile(loss='mse', optimizer=Adam(lr=0.00001))\n",
    "        critic.summary()\n",
    "        return critic\n",
    "\n",
    "    def get_action(self, state):\n",
    "        self.epsilon = max(self.epsilon*self.epsilon_decay,0.05)\n",
    "        #print(state)\n",
    "        action = self.actor.predict(state)\n",
    "\n",
    "        real = action + self.epsilon*self.noiser.noise()\n",
    "        return np.clip(real,-1.1,1.1)\n",
    "    \n",
    "    def gat_action_nonoise(self,state):\n",
    "        action = self.actor.predict(state)\n",
    "        \n",
    "        real = action + 0.1*self.noiser.noise()\n",
    "        return np.clip(real,-1.1,1.1)\n",
    "\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        for i in range(self.agent_size):\n",
    "            self.memory.append((state[i], action[i], reward[i], next_state[i], done[i]))\n",
    "        #self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train_model(self):\n",
    "        # make mini-batch from replay memory\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        states = np.asarray([e[0] for e in mini_batch])\n",
    "        actions = np.asarray([e[1] for e in mini_batch])\n",
    "        rewards = np.asarray([e[2] for e in mini_batch])\n",
    "        next_states = np.asarray([e[3] for e in mini_batch])\n",
    "        dones = np.asarray([e[4] for e in mini_batch])\n",
    "\n",
    "        # update critic network\n",
    "        critic_action_input = self.actor_target.predict(next_states)\n",
    "        target_q_values = self.critic_target.predict([next_states, critic_action_input])\n",
    "\n",
    "        targets = np.zeros([self.batch_size, 1])\n",
    "        for i in range(self.batch_size):\n",
    "            if dones[i]:\n",
    "                targets[i] = rewards[i]\n",
    "            else:\n",
    "                targets[i] = rewards[i] + self.discount_factor * target_q_values[i]\n",
    "\n",
    "        self.critic.train_on_batch([states, actions], targets)\n",
    "\n",
    "        # update actor network\n",
    "        a_for_grad = self.actor.predict(states)\n",
    "        self.actor_updater([states, states, a_for_grad])\n",
    "        #self.actor_updater([states, states, actions])\n",
    "        \n",
    "    def train_critic(self):\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        states = np.asarray([e[0] for e in mini_batch])\n",
    "        actions = np.asarray([e[1] for e in mini_batch])\n",
    "        rewards = np.asarray([e[2] for e in mini_batch])\n",
    "        next_states = np.asarray([e[3] for e in mini_batch])\n",
    "        dones = np.asarray([e[4] for e in mini_batch])\n",
    "        \n",
    "        self.critic.train_on_batch([states, actions], rewards)\n",
    "\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.actor_target.set_weights(self.actor.get_weights())\n",
    "        self.critic_target.set_weights(self.critic.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent state looks like: \n",
      "[-0.08413965  0.50443733  0.          3.37213826  0.        ]\n",
      "Agent shape looks like: \n",
      "(10, 5)\n",
      "Agent shape looks like: \n",
      "(0,)\n",
      "building actor network\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                384       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 25,409\n",
      "Trainable params: 25,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "building actor network\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                384       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 25,409\n",
      "Trainable params: 25,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "building critic network\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "state_input (InputLayer)        (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "action_input (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 64)           384         state_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 64)           128         action_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 64)           4160        dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 64)           4160        dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           dense_18[0][0]                   \n",
      "                                                                 dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 64)           8256        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 64)           4160        dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 64)           4160        dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 64)           4160        dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 64)           4160        dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 64)           4160        dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 64)           4160        dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 1)            65          dense_27[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 42,113\n",
      "Trainable params: 42,113\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "building critic network\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "state_input (InputLayer)        (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "action_input (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 64)           384         state_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 64)           128         action_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 64)           4160        dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 64)           4160        dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128)          0           dense_30[0][0]                   \n",
      "                                                                 dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 64)           8256        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 64)           4160        dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 64)           4160        dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 64)           4160        dense_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 64)           4160        dense_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 64)           4160        dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 64)           4160        dense_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 1)            65          dense_39[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 42,113\n",
      "Trainable params: 42,113\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=True)[default_brain]\n",
    "\n",
    "# Examine the state space for the default brain\n",
    "print(\"Agent state looks like: \\n{}\".format(env_info.vector_observations[0]))\n",
    "\n",
    "print(\"Agent shape looks like: \\n{}\".format(np.shape(env_info.vector_observations)))\n",
    "\n",
    "for observation in env_info.visual_observations:\n",
    "    print(\"Agent observations look like:\")\n",
    "    if observation.shape[3] == 3:\n",
    "        plt.imshow(observation[0,:,:,:])\n",
    "    else:\n",
    "        plt.imshow(observation[0,:,:,0])\n",
    "print(\"Agent shape looks like: \\n{}\".format(np.shape(env_info.visual_observations)))\n",
    "\n",
    "agent = DDPGAgent(5,10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_0 reward: 10.479934573173523 episilon: 0.9990004948383437\n",
      "episode_1 reward: 7.9904765605926515 episilon: 0.9980019886872552\n",
      "episode_2 reward: 4.530595290660858 episilon: 0.9970044805482192\n",
      "episode_3 reward: 4.48782849907875 episilon: 0.9960079694237177\n",
      "episode_4 reward: 4.250889068841934 episilon: 0.9950124543172282\n",
      "episode_5 reward: 4.164966660737991 episilon: 0.994017934233226\n",
      "episode_6 reward: 4.645557117462158 episilon: 0.9930244081771812\n",
      "episode_7 reward: 5.114448535442352 episilon: 0.9920318751555576\n",
      "episode_8 reward: 3.975300818681717 episilon: 0.9910403341758119\n",
      "episode_9 reward: 3.776609319448471 episilon: 0.9900497842463938\n",
      "episode_10 reward: 3.7959738254547117 episilon: 0.9890602243767426\n",
      "episode_11 reward: 4.625797080993652 episilon: 0.9880716535772887\n",
      "episode_12 reward: 4.52214515209198 episilon: 0.9870840708594518\n",
      "episode_13 reward: 4.1273159146308895 episilon: 0.9860974752356391\n",
      "episode_14 reward: 3.5458719551563265 episilon: 0.9851118657192448\n",
      "episode_15 reward: 28.852427816390993 episilon: 0.9841272413246489\n",
      "episode_16 reward: 49.171500337123874 episilon: 0.9831436010672185\n",
      "episode_17 reward: 86.97065617442131 episilon: 0.9821609439633026\n",
      "episode_18 reward: 46.16661171913147 episilon: 0.9811792690302334\n",
      "episode_19 reward: 26.78249876499176 episilon: 0.980198575286327\n",
      "episode_20 reward: 19.728674989938735 episilon: 0.9792188617508799\n",
      "episode_21 reward: 8.425874668359757 episilon: 0.9782401274441684\n",
      "episode_22 reward: 15.625612580776215 episilon: 0.9772623713874483\n",
      "episode_23 reward: 10.030520421266555 episilon: 0.9762855926029546\n",
      "episode_24 reward: 22.58028112053871 episilon: 0.9753097901138975\n",
      "episode_25 reward: 32.1716529250145 episilon: 0.9743349629444651\n",
      "episode_26 reward: 28.827513712644578 episilon: 0.9733611101198196\n",
      "episode_27 reward: 27.558262985944747 episilon: 0.9723882306660994\n",
      "episode_28 reward: 23.72173942923546 episilon: 0.9714163236104145\n",
      "episode_29 reward: 30.99329542517662 episilon: 0.970445387980849\n",
      "episode_30 reward: 49.252739787101746 episilon: 0.9694754228064565\n",
      "episode_31 reward: 43.14850848913193 episilon: 0.9685064271172623\n",
      "episode_32 reward: 38.572794103622435 episilon: 0.9675383999442618\n",
      "episode_33 reward: 37.570308005809785 episilon: 0.9665713403194166\n",
      "episode_34 reward: 26.0297091960907 episilon: 0.9656052472756584\n",
      "episode_35 reward: 35.05682743191719 episilon: 0.9646401198468838\n",
      "episode_36 reward: 36.9933367729187 episilon: 0.9636759570679563\n",
      "episode_37 reward: 36.02483164668083 episilon: 0.9627127579747022\n",
      "episode_38 reward: 43.38145805001259 episilon: 0.961750521603914\n",
      "episode_39 reward: 41.01292110681534 episilon: 0.9607892469933452\n",
      "episode_40 reward: 41.72661213278771 episilon: 0.9598289331817111\n",
      "episode_41 reward: 35.86948421597481 episilon: 0.9588695792086886\n",
      "episode_42 reward: 38.636923998594284 episilon: 0.9579111841149139\n",
      "episode_43 reward: 45.67014010548591 episilon: 0.9569537469419825\n",
      "episode_44 reward: 36.367066246271136 episilon: 0.9559972667324481\n",
      "episode_45 reward: 31.887554043531416 episilon: 0.9550417425298197\n",
      "episode_46 reward: 46.182200694084166 episilon: 0.9540871733785639\n",
      "episode_47 reward: 42.51575021147728 episilon: 0.9531335583241021\n",
      "episode_48 reward: 36.77745455503464 episilon: 0.9521808964128091\n",
      "episode_49 reward: 33.56064303517341 episilon: 0.9512291866920142\n",
      "model saved\n",
      "episode_50 reward: 44.89399150013924 episilon: 0.9502784282099976\n",
      "episode_51 reward: 40.3840990126133 episilon: 0.9493286200159913\n",
      "episode_52 reward: 41.945526105165484 episilon: 0.9483797611601773\n",
      "episode_53 reward: 37.862584692239764 episilon: 0.947431850693687\n",
      "episode_54 reward: 34.87157269120216 episilon: 0.9464848876686018\n",
      "episode_55 reward: 47.04828264713287 episilon: 0.9455388711379469\n",
      "episode_56 reward: 35.98752329349518 episilon: 0.9445938001556984\n",
      "episode_57 reward: 33.39783553481102 episilon: 0.9436496737767741\n",
      "episode_58 reward: 46.89633005857468 episilon: 0.9427064910570385\n",
      "episode_59 reward: 43.22303075194359 episilon: 0.9417642510533004\n",
      "episode_60 reward: 43.25729162693024 episilon: 0.9408229528233091\n",
      "episode_61 reward: 44.48759598135948 episilon: 0.9398825954257575\n",
      "episode_62 reward: 33.04481793642044 episilon: 0.9389431779202787\n",
      "episode_63 reward: 22.585865712165834 episilon: 0.9380046993674448\n",
      "episode_64 reward: 37.68031005263329 episilon: 0.9370671588287687\n",
      "episode_65 reward: 37.07975743412972 episilon: 0.9361305553667009\n",
      "episode_66 reward: 43.50172281265259 episilon: 0.9351948880446276\n",
      "episode_67 reward: 47.06904194951058 episilon: 0.9342601559268722\n",
      "episode_68 reward: 46.73969616293907 episilon: 0.9333263580786937\n",
      "episode_69 reward: 51.08250102996826 episilon: 0.932393493566284\n",
      "episode_70 reward: 47.969495576620105 episilon: 0.9314615614567696\n",
      "episode_71 reward: 29.90583337545395 episilon: 0.9305305608182091\n",
      "episode_72 reward: 41.519311010837555 episilon: 0.9296004907195919\n",
      "episode_73 reward: 43.76010709404945 episilon: 0.9286713502308396\n",
      "episode_74 reward: 43.18401384353638 episilon: 0.9277431384228018\n",
      "episode_75 reward: 53.78122492432594 episilon: 0.9268158543672568\n",
      "episode_76 reward: 39.3933136343956 episilon: 0.9258894971369117\n",
      "episode_77 reward: 41.14873924255371 episilon: 0.9249640658054001\n",
      "episode_78 reward: 48.37593668103218 episilon: 0.9240395594472816\n",
      "episode_79 reward: 50.34864318370819 episilon: 0.9231159771380392\n",
      "episode_80 reward: 35.83670797348022 episilon: 0.9221933179540824\n",
      "episode_81 reward: 40.78344211578369 episilon: 0.9212715809727428\n",
      "episode_82 reward: 39.978206515312195 episilon: 0.9203507652722732\n",
      "episode_83 reward: 47.34651573300361 episilon: 0.9194308699318492\n",
      "episode_84 reward: 39.02034488916397 episilon: 0.9185118940315662\n",
      "episode_85 reward: 43.96294741034508 episilon: 0.9175938366524392\n",
      "episode_86 reward: 45.27187021374702 episilon: 0.9166766968764013\n",
      "episode_87 reward: 52.27953736782074 episilon: 0.9157604737863031\n",
      "episode_88 reward: 46.381780463457105 episilon: 0.9148451664659124\n",
      "episode_89 reward: 44.11233973503113 episilon: 0.9139307739999132\n",
      "episode_90 reward: 49.10155177116394 episilon: 0.9130172954739034\n",
      "episode_91 reward: 50.45521042943001 episilon: 0.912104729974396\n",
      "episode_92 reward: 47.22163711190224 episilon: 0.9111930765888158\n",
      "episode_93 reward: 45.2626395702362 episilon: 0.9102823344055\n",
      "episode_94 reward: 49.56447647809982 episilon: 0.909372502513697\n",
      "episode_95 reward: 45.195412528514865 episilon: 0.9084635800035663\n",
      "episode_96 reward: 41.056016492843625 episilon: 0.907555565966176\n",
      "episode_97 reward: 48.57892053723335 episilon: 0.9066484594935031\n",
      "episode_98 reward: 48.114666080474855 episilon: 0.9057422596784318\n",
      "episode_99 reward: 45.94767149686813 episilon: 0.9048369656147537\n",
      "model saved\n",
      "episode_100 reward: 55.95660873055458 episilon: 0.9039325763971644\n",
      "episode_101 reward: 58.89987705945968 episilon: 0.9030290911212665\n",
      "episode_102 reward: 49.15104418992996 episilon: 0.902126508883565\n",
      "episode_103 reward: 49.1800156891346 episilon: 0.9012248287814689\n",
      "episode_104 reward: 62.19663689136505 episilon: 0.900324049913289\n",
      "episode_105 reward: 54.48222088813782 episilon: 0.8994241713782375\n",
      "episode_106 reward: 40.97334100008011 episilon: 0.8985251922764269\n",
      "episode_107 reward: 45.80793862938881 episilon: 0.8976271117088688\n",
      "episode_108 reward: 54.0733159840107 episilon: 0.8967299287774732\n",
      "episode_109 reward: 55.90031355023384 episilon: 0.8958336425850486\n",
      "episode_110 reward: 57.69296782016754 episilon: 0.8949382522352994\n",
      "episode_111 reward: 52.582048910856244 episilon: 0.8940437568328268\n",
      "episode_112 reward: 58.2995501935482 episilon: 0.8931501554831258\n",
      "episode_113 reward: 67.78863478302955 episilon: 0.8922574472925864\n",
      "episode_114 reward: 79.77953293323517 episilon: 0.891365631368491\n",
      "episode_115 reward: 68.64113075733185 episilon: 0.8904747068190155\n",
      "episode_116 reward: 71.33531058430671 episilon: 0.8895846727532252\n",
      "episode_117 reward: 72.36469466090202 episilon: 0.8886955282810781\n",
      "episode_118 reward: 68.76730487942696 episilon: 0.8878072725134205\n",
      "episode_119 reward: 68.47307842969894 episilon: 0.886919904561988\n",
      "episode_120 reward: 71.87393523454666 episilon: 0.8860334235394025\n",
      "episode_121 reward: 76.12549570202827 episilon: 0.8851478285591758\n",
      "episode_122 reward: 76.30873135328292 episilon: 0.8842631187357024\n",
      "episode_123 reward: 88.68881441950798 episilon: 0.883379293184264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_124 reward: 79.30319168567658 episilon: 0.8824963510210262\n",
      "episode_125 reward: 75.67915295362472 episilon: 0.8816142913630383\n",
      "episode_126 reward: 106.38317249417305 episilon: 0.8807331133282309\n",
      "episode_127 reward: 74.30730329751968 episilon: 0.8798528160354181\n",
      "episode_128 reward: 66.67981781363487 episilon: 0.8789733986042928\n",
      "episode_129 reward: 84.30796032547951 episilon: 0.878094860155429\n",
      "episode_130 reward: 94.7100695848465 episilon: 0.8772171998102793\n",
      "episode_131 reward: 90.92554764151573 episilon: 0.8763404166911752\n",
      "episode_132 reward: 104.68920612931251 episilon: 0.8754645099213243\n",
      "episode_133 reward: 89.08413852453232 episilon: 0.8745894786248111\n",
      "episode_134 reward: 85.58658061623574 episilon: 0.8737153219265954\n",
      "episode_135 reward: 99.37981079220772 episilon: 0.8728420389525116\n",
      "episode_136 reward: 104.57235450744629 episilon: 0.8719696288292675\n"
     ]
    }
   ],
   "source": [
    "train_mode =  True  # Whether to run the environment in training or inference mode\n",
    "\n",
    "reward_memory = deque(maxlen=20)\n",
    "agents_reward = np.zeros(agent.agent_size)\n",
    "env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "for episode in range(10000000):\n",
    "    #env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "    state = env_info.vector_observations\n",
    "    done = False\n",
    "    episode_rewards = 0\n",
    "    for i in range(100):\n",
    "        \n",
    "        if not train_mode:\n",
    "            action = agent.get_action(state)\n",
    "        else:\n",
    "            action = agent.get_action(state)\n",
    "        #action = np.column_stack([np.random.randint(0, action_size[i], size=(len(env_info.agents))) for i in range(len(action_size))])\n",
    "        env_info = env.step(action)[default_brain]\n",
    "        next_state = env_info.vector_observations\n",
    "        reward = env_info.rewards\n",
    "        agents_reward += reward\n",
    "        #episode_rewards += reward#env_info.rewards[0]\n",
    "        done = env_info.local_done\n",
    "        for idx,don in enumerate(done):\n",
    "            if don:\n",
    "                reward_memory.append(agents_reward[idx])\n",
    "                agents_reward[idx] = 0\n",
    "        if train_mode:\n",
    "            agent.append_sample(state,action,reward,next_state,done)\n",
    "        state = next_state\n",
    "        \n",
    "        if len(agent.memory) > agent.batch_size and train_mode:\n",
    "            agent.train_model()\n",
    "        \n",
    "    #agent.noiser.reset()\n",
    "    agent.update_target_model()\n",
    "    if agent.epsilon <= 0.05:\n",
    "        agent.epsilon = 0.99\n",
    "\n",
    "    if episode%50 == 0 and not episode == 0 and train_mode:\n",
    "        \n",
    "        agent.actor.save(\"3DBall_actor.h5\")\n",
    "        agent.critic.save(\"3DBall_critic.h5\")\n",
    "        print(\"model saved\")\n",
    "    print(\"episode_{} reward: {} episilon: {}\".format(episode,np.mean(reward_memory),agent.epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0120590925216675, 1.0189833641052246, 1.0423592329025269, 1.014908790588379, 1.0127912759780884, 1.2123252153396606, 1.0106924772262573, 1.0095158815383911, 1.0150420665740967, 1.0495394468307495]\n"
     ]
    }
   ],
   "source": [
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.epsilon_decay =0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
